---
title: "Home based anxiety interventions meta-analysis"
author: "Tamas Nagy"
date: "6/29/2020"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
editor_options: 
  chunk_output_type: console
csl: apa.csl
bibliography: [references.bib]
---

CHANGES:  

- Included a 2020 study
- Added all analysis concerning parent ratings
- Refactored some variable names for clarity (selfrep -> self, rating - > clinician)

# Prepare environment and data

```{r Packages & settings, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(encoding = 'UTF-8')

library(tidyverse)
library(readxl)
library(metafor)
library(metaviz)
library(dmetar)
library(janitor)
library(citr)
library(osfr)
library(PRISMAstatement)
library(patchwork)
library(here)

```

```{r Prep bibliography, eval=FALSE, include=FALSE}
# This does not have to be run
# Creates a tidy bibliography from the local bib file
# citr::tidy_bib_file(messy_bibliography = "d:/Documents/Mendeley references/library.bib",
#                     file = "references.bib", 
#                     rmd_file = "index.Rmd",
#                     encoding = "UTF-8")

```

```{r Download data from osf, results = "hide", message=FALSE, warning=FALSE}
# Authenticate using a PAT
osf_pat <- read_lines("osf_pat.txt")
osf_auth(token = osf_pat)

# Retrieve data to data folder
osf_file <- 
  osf_retrieve_file("bqy2p") %>%
  osf_download(path = "data/",
               conflicts = "overwrite",
               progress = TRUE)


```

# Analysis


```{r Load data}
cbt <-
  read_excel(here::here(osf_file$local_path), na = c("", "N/A")) %>%
  clean_names() %>% 
  # Get the female ratio
  mutate(female = map_dbl(female_n_total_n, ~eval(parse(text = .x))),
         sessions_completed_totalsessions = str_replace_all(sessions_completed_totalsessions, ",", "."),
         completed_sessions = map_dbl(sessions_completed_totalsessions, 
                                      ~eval(parse(text = .x))))

```

The meta-analyses models were recalculated using REML estimation. Using this only slghtly changes the results, but IMO it is a more appropriate and defensible choice  @Viechtbauer2005.

```{r Build meta models}
# Model for the children self-report MA
self_df <- 
  cbt %>% 
  filter(outcome == "children") 

mod_self <-  
  rma(yi = hedges_g, 
      sei = hedges_se, 
      data = self_df, 
      method = "REML")

mod_self


# Model for the clinician rating MA
clinician_df <- 
  cbt %>%
  filter(outcome == "clinician")

mod_clinician <-
  rma(yi = hedges_g,
      sei = hedges_se, 
      data = clinician_df,
      method = "REML")

mod_clinician

# Model for the clinician rating MA
parent_df <- 
  cbt %>%
  filter(outcome == "parental")

mod_parent <-
  rma(yi = hedges_g,
      sei = hedges_se, 
      data = parent_df,
      method = "REML")

mod_parent

```

## Outlier detection

Studies that has a confidence interval completely outside of the aggregated effect's CI should be considered outliers. We identified the Cobham et al. 2012 study as an outlier for both the self-report and the clincian rated outcomes, so we removed this study, and recalculated the meta-analyses. No outliers in the parental rating studies.

```{r outlier detection}
# Self-report
find.outliers(mod_self)
# Clinician rating
find.outliers(mod_clinician)
# Clinician rating
find.outliers(mod_parent)
```


```{r recalculate MA without outliers}
# Remove outliers from self-report data
self_df <- 
  cbt %>% 
  filter(outcome == "children") %>% 
  slice(-1)

# Corrected model for self-report data
mod_self <-  
  rma(yi = hedges_g, 
           sei = hedges_se, 
           data = self_df, 
           method = "REML")

find.outliers(mod_self)

# No more outliers!
# Remove outliers from clinician-rating data
clinician_df <-   
  cbt %>% 
  filter(outcome == "clinician") %>% 
  slice(-1)

# Model for the clinician rating MA
mod_clinician <-
  rma(yi = hedges_g,
      sei = hedges_se, 
      data = clinician_df,
      method = "REML")

# No further outliers
find.outliers(mod_clinician)

```

## Final models (without outliers)
```{r}
mod_self
mod_clinician
mod_parent
```


# Forest plots
## Self-report

```{r tables will be added}
# Create tables that will be added to the forest plots
# Self-rating
self_table <-
  self_df %>% 
  select(Study = study_name, N = sample_size_total)

self_summary <- 
  self_table %>% 
  group_by(name = "Aggregated effect") %>% 
  summarise(N = sum(N))

# Clinician rating
clinician_table <-
  clinician_df %>% 
  select(Study = study_name, N = sample_size_total)

clinician_summary <- 
  clinician_table %>% 
  group_by(name = "Aggregated effect") %>% 
  summarise(N = sum(N))

# Parental rating
parent_table <-
  parent_df %>% 
  select(Study = study_name, N = sample_size_total)

parent_summary <- 
  parent_table %>% 
  group_by(name = "Aggregated effect") %>% 
  summarise(N = sum(N))

```

## Self-report
```{r fig.width = 9}
viz_forest(mod_self, 
           study_table = self_table,
           text_size = 4, 
           annotate_CI = TRUE, 
           xlab = "Hedges's g", 
           summary_table = self_summary)

ggsave(filename = here("figures/fig2_forest_self-report.png"), width = 13, height = 7)

```

## Clinician rating
```{r fig.width = 9, warning = FALSE}
viz_forest(mod_clinician, 
           study_table = clinician_table,
           text_size = 3.5, 
           xlab = "Hedges'g", 
           summary_table = clinician_summary,
           annotate_CI = TRUE)

ggsave(filename = here("figures/fig3_forest_clinician.png"), width = 13, height = 7)
```

## Parental rating
```{r fig.width = 9, warning = FALSE}
viz_forest(mod_parent, 
           study_table = parent_table,
           text_size = 3.5, 
           xlab = "Hedges'g", 
           summary_table = parent_summary,
           annotate_CI = TRUE)

ggsave(filename = here("figures/fig4_forest_parent.png"), width = 13, height = 7)
```

# Funnel plots
## Self-report

As there seems to be some publication or small study bias, we can try to correct for that. Self-report, clinician-rating, and parent-rating models were corrected by 4, 4, and 3 TAF studies, respectively.

All models remained significant after trim-and-fill imputed studies. Also, the if we compare the overall effect size with TAF studies included, the effects remain similar.


```{r}
# trimandfill tails for the first model, while it work in the funnel plot 
trimfill(self_df$hedges_g,
         self_df$hedges_se,
         ma.fixed = FALSE,
         method.tau = "REML")

# Egger's test
regtest(mod_self, model = "lm")

funnel_self <- 
  viz_funnel(mod_self, 
             method = "RE",
             sig_contours = TRUE, 
             contours = TRUE,
             trim_and_fill = TRUE,
             text_size = 4) +
  labs(title = "Self-report")

ggsave(plot = funnel_self, filename = here("figures/fig_funnel_self-report.png"), width = 13, height = 7)

```

## Clinician rating

```{r}
trimfill(clinician_df$hedges_g,
         clinician_df$hedges_se,
         ma.fixed = FALSE,
         method.tau = "REML")

# Egger's test
regtest(mod_clinician, model = "lm")

funnel_clinician <- 
viz_funnel(mod_clinician, 
           method = "RE",
           sig_contours = TRUE, 
           trim_and_fill = TRUE,
           contours = TRUE,
           text_size = 4) +
  labs(title = "Clinician rated")

ggsave(plot = funnel_clinician, filename = here("figures/fig_funnel_clinician.png"), width = 13, height = 7)

```

## Parental rating

```{r}
trimfill(parent_df$hedges_g,
         parent_df$hedges_se,
         ma.fixed = FALSE,
         method.tau = "REML")

# Egger's test
regtest(mod_parent, model = "lm")

funnel_parent <- 
  viz_funnel(mod_parent, 
             method = "RE",
             sig_contours = TRUE, 
             trim_and_fill = TRUE,
             contours = TRUE,
             text_size = 4) +
  labs(title = "Parent rated")

ggsave(plot = funnel_parent, filename = here("figures/fig_funnel_parent.png"), width = 13, height = 7)

```

# Combine funnel plots
```{r}

funnel_self + funnel_clinician + funnel_parent

```



# Moderation analyses

## Self-report

```{r}
# Intervention characteristics
## Family or youth focus (focus)
rma.uni(hedges_g, sei = hedges_se, data = self_df, mods = ~focus) %>% 
  anova()
## Therapist involvement (therapist_involvement)
rma.uni(hedges_g, sei = hedges_se, data = self_df, 
        mods = ~therapist_involvement) %>% 
  anova()

## Place of delivery
rma.uni(hedges_g, sei = hedges_se, data = self_df, mods = ~placeof_delivery) %>% anova()
## Average length
rma.uni(hedges_g, sei = hedges_se, data = self_df, mods = ~completed_sessions)  %>% anova()

# Study characteristics
## Study quality
rma.uni(hedges_g, sei = hedges_se, data = self_df, mods = ~risk_of_bias) %>% anova()
## Anxiety measure
rma.uni(hedges_g, sei = hedges_se, data = self_df, mods = ~anxiety_measure) %>%  anova()
## Intended length
rma.uni(hedges_g, sei = hedges_se, data = self_df, mods = ~intervention_length_n_of_modules) %>% anova()

# Participant characteristics
## Age
rma.uni(hedges_g, sei = hedges_se, data = self_df, mods = ~avg_age_pop) %>% anova()
## Female ratio
rma.uni(hedges_g, sei = hedges_se, data = self_df, mods = ~female) %>% anova()
## Clinical status
rma.uni(hedges_g, sei = hedges_se, data = self_df, mods = ~clinical_status) %>% anova()

```

## Clinician rating
```{r}
## Method
rma.uni(hedges_g, sei = hedges_se, data = clinician_df, mods = ~method) %>% anova()
## Family or youth focus (focus)
rma.uni(hedges_g, sei = hedges_se, data = clinician_df, mods = ~focus) %>% anova()
## Therapist involvement (therapist_involvement)
rma.uni(hedges_g, sei = hedges_se, data = clinician_df, mods = ~therapist_involvement) %>% anova()
## Clinical status
rma.uni(hedges_g, sei = hedges_se, data = clinician_df, mods = ~clinical_status) %>% anova()
## Place of delivery
rma.uni(hedges_g, sei = hedges_se, data = clinician_df, mods = ~placeof_delivery) %>% anova()
## Intended length
rma.uni(hedges_g, sei = hedges_se, data = clinician_df, mods = ~intervention_length_n_of_modules) %>% anova()
## Study quality
rma.uni(hedges_g, sei = hedges_se, data = clinician_df, mods = ~risk_of_bias) %>% anova()
## Clinician blinding
rma.uni(hedges_g, sei = hedges_se, data = clinician_df, mods = ~clinician_blinding) %>% anova()

rma.uni(hedges_g, sei = hedges_se, data = clinician_df, mods = ~intervention_length_n_of_modules) %>% anova()
## Average length
rma.uni(hedges_g, sei = hedges_se, data = clinician_df, mods = ~completed_sessions) %>% anova()
## Age
rma.uni(hedges_g, sei = hedges_se, data = clinician_df, mods = ~avg_age_pop) %>% anova()
## Female ratio
rma.uni(hedges_g, sei = hedges_se, data = clinician_df, mods = ~female) %>% anova()

```

## Parent rating
```{r}
## Method is significant!
rma.uni(hedges_g, sei = hedges_se, data = parent_df, mods = ~method) %>% anova()

rma.uni(hedges_g, sei = hedges_se, data = parent_df, mods = ~method) %>% 
  metaviz::viz_forest()

## Family or youth focus (focus)
rma.uni(hedges_g, sei = hedges_se, data = parent_df, mods = ~focus) %>% anova()
## Therapist involvement (therapist_involvement)
rma.uni(hedges_g, sei = hedges_se, data = parent_df, mods = ~therapist_involvement) %>% anova()
## Clinical status
rma.uni(hedges_g, sei = hedges_se, data = parent_df, mods = ~clinical_status) %>% anova()
## Place of delivery
rma.uni(hedges_g, sei = hedges_se, data = parent_df, mods = ~placeof_delivery) %>% anova()
## Intended length
rma.uni(hedges_g, sei = hedges_se, data = parent_df, mods = ~intervention_length_n_of_modules) %>% anova()
## Study quality
rma.uni(hedges_g, sei = hedges_se, data = parent_df, mods = ~risk_of_bias) %>% anova()
rma.uni(hedges_g, sei = hedges_se, data = parent_df, mods = ~intervention_length_n_of_modules) %>% anova()
## Average length
rma.uni(hedges_g, sei = hedges_se, data = parent_df, mods = ~completed_sessions) %>% anova()
## Age
rma.uni(hedges_g, sei = hedges_se, data = parent_df, mods = ~avg_age_pop) %>% anova()
## Female ratio
rma.uni(hedges_g, sei = hedges_se, data = parent_df, mods = ~female) %>% anova()

```

# Dropout analysis
```{r eval=FALSE, include=FALSE}
self_df %>% 
  select(study_name, drop_out_int, drop_out_cont) %>% 
  pivot_longer(-study_name,
               names_to = "outcome",
               values_to = "group") %>% 
  group_by(outcome) %>% 
  summarise(avg_dropout = mean(group, na.rm = TRUE),
            med_dropout = median(group, na.rm = TRUE))

  
clinician_df %>% 
  select(study_name, drop_out_int, drop_out_cont) %>% 
  pivot_longer(-study_name,
               names_to = "outcome",
               values_to = "group") %>% 
  group_by(outcome) %>% 
  summarise(avg_dropout = mean(group),
            med_dropout = median(group))

```

## Dropouts and intervention length association
## All dropouts as moderator


# Flowcart
WIP
```{r eval=FALSE, include=FALSE}

prisma_graph(found = 3113, 
             found_other = 0,
             screened = 431,
             screen_exclusions = 0,
             no_dupes = 10,
             full_text = 0, 
             full_text_exclusions = 0, 
             qualitative = 0, 
             quantitative = 0)

grViz("digraph flowchart {
      # node definitions with substituted label text
      node [fontname = Helvetica, shape = rectangle]  
      cat1 [label = 'Screening']
      
      node [fontname = Helvetica, shape = rectangle]        
      tab1 [label = 'Papers identified by database searches \\n k = 3113']
      tab2 [label = 'Papers screened based on title and abstract \\n k = 431']
      tab3 [label = 'Full text assessed for eligibility\\n k = 128']
      tab4 [label = 'Papers included for meta-analysis\\n k = 18']
      tab5 [label = 'Papers excluded based on title\\n k = 2682']
      tab6 [label = 'Papers excluded based on abstract\\n k = 303']
      tab7 [label = 'Papers excluded after full text assessed\\n k = 110']

      # edge definitions with the node IDs
      cat1
      tab1 -> tab2 -> tab3 -> tab4
      tab1 -> tab5
      tab2 -> tab6
      tab3 -> tab7
      }


      ")
```


# References  
